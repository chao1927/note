## 三、Hadoop 安装与配置

0. 安装前置条件

     1. ##### JDK 的安装与配置

         下载 jdk

         ```shell
     wget https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html
         ```

         解压

         ```shell
      tar -zxvf jdk-xxx.tar.gz
         ```
         
         设置软链接 
         
         ```shell
     ln -s  ${java_tar_dir}  /usr/local/java
         ```

         设置环境变量

         ```shell
         vim /etc/profile
         ```
         
         ```
         export JAVA_HOME=/usr/local/java
         export PATH=$JAVA_HOME/bin:$PATH
         export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
         ```
         
         source 配置文件使其生效
         
         ```shell
         source -e /etc/profile
         ```
         
         校验是否安装成功
         
         ```shell
         java -version 
         ```
         
         

     2. ##### host 配置

     ```
     192.168.195.129 node1
     192.168.195.130 node2
     192.168.195.131 node3
     ```

     

     3. ##### SSH免密登录配置

     - 本地客户端生成公私钥：（一路回车默认即可）

     ```shell
     ssh-keygen -t rsa -P ""xxxxxxxxxx1 1ssh-keygenssh-keygen -t rsa -P ""
     ```

     - 上面这个命令会在用户目录.ssh文件夹下创建公私钥

     ```shell
     cd ~/.ssh
     ls
     ```

     - ~/.ssh 下增加了两个文件

     ```shell
     id_rsa （私钥）
     id_rsa.pub (公钥)
     ```

     - 上传公钥到服务器

     ```
     ssh-copy-id -i ~/.ssh/id_rsa.pub {targetUser}@{targetHost}
     ```

     - 将公钥追加到 **authorized_keys**

     ```shell
     cat id_rsa.pub >> authorized_keys
     ```

     - ~/.ssh 目录文件

     ```shell
     authorized_keys文件存放其他主机的公钥，其他主机即可ssh登录该机
     know_hosts记录主机登陆过的其他主机的公钥信息
     ```

     

     4. #### Hadoop 安装部署

     集群部署规划

     ![Hadoop集群部署](D:%5Cnote%5Chadoop%5CHadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.png)

     

     准备工作

     - 关闭防火墙

     ```shell
     sudo systemctl stop firewalld
     sudo systemctl disable firewalld
     ```

     - 禁用 selinux

     ```shell
     sudo vim /etc/selinux/config
     SELINUX=enforcing --> SELINUX=disabled
     ```

     - 配置时间同步

     ```shell
     sudo yum -y install ntp ntpdate 
     ntpd  ntp.aliyun.com 
     ```

     

     

     1. 软件包下载与解压 

         ```shell
         # 可下载 3.x 版本 Hadoop
         wget https://archive.apache.org/dist/hadoop/common/hadoop-2.9.2/
         tar -zxvf hadoop-2.9.2.tar.gz -C {指定安装目标}
         ```

         

     2. 配置环境变量

         - 添加Hadoop到环境变量 vim /etc/profile

         ```shell
         ##HADOOP_HOME
         export HADOOP_HOME=/opt/lagou/servers/hadoop-2.9.2
         export PATH=$PATH:$HADOOP_HOME/bin
         export PATH=$PATH:$HADOOP_HOME/sbin
         ```

         - 使环境变量生效

         ```shell
         source /etc/profile
         ```

         - 验证hadoop

         ```shell
         hadoop version
         ```

         

     3. Hadoop 集群配置

         1. HDFS 集群配置

             - hadoop-env.sh

             ```shell
             #指定JAVA_HOME
             export JAVA_HOME=/opt/soft/jdk1.8.0\_251 
             #指定hadoop用户，hadoop3.x之后必须配置(我的用户名就叫hadoop)
             export HDFS_NAMENODE_USER=hadoop
             export HDFS_DATANODE_USER=hadoop 
             export HDFS_ZKFC_USER=hadoop 
             export HDFS_JOURNALNODE_USER=hadoop
             export YARN_RESOURCEMANAGER_USER=hadoop 
             export YARN_NODEMANAGER_USER=hadoop
             ```

             - core-site.xml

             ```shell
             <!--集群名称-->
             <property> 
                 <name>fs.defaultFS</name> 
                 <value>hdfs://node2:9000</value> 
             </property> 
             <!--临时目录-->
             <property> 
                 <name>hadoop.tmp.dir</name> 
                 <value>/usr/local/hadoop/data</value> 
             </property>
             <!--webUI展示时的用户-->
             <property> 
                 <name>hadoop.http.staticuser.user</name>            
                 <value>hadoop</value> 
             </property>
             ```

             - hdfs-site.xml

             ```shell
             <property>
                 <name>dfs.namenode.secondary.http-address</name>
                 <value>node3:50090</value>
             </property>
             
             <!--副本数量 -->
             <property>
                 <name>dfs.replication</name>
                 <value>3</value>
             </property>
             ```

             - 配置 workers

             ```shell
             node1
             node2
             node3
             ```

             

             ##### 2. MapReduce 配置

             - mapred-site.xml

             ```shell
             <property>
                 <name>mapreduce.framework.name</name>
                 <value>yarn</value>
             </property>
             
             <property>
               <name>yarn.app.mapreduce.am.env</name>
               <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
             </property>
             <property>
               <name>mapreduce.map.env</name>
               <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
             </property>
             <property>
               <name>mapreduce.reduce.env</name>
               <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
             </property>
             ```

             3. Yarn 集群配置

             - yarn-site.xml

             ```shell
             <!-- 指定YARN的ResourceManager的地址 -->
             <property>
                 <name>yarn.resourcemanager.hostname</name>
                 <value>node3</value>
             </property>
             <!-- Reducer获取数据的方式 -->
             <property>
                 <name>yarn.nodemanager.aux-services</name>
                 <value>mapreduce_shuffle</value>
             </property>
             ```

     4. 分发集群配置

     ```shell
     rsync /usr/local/hadoop root@node1:/usr/local/hadoop/
     rsync /usr/local/hadoop root@node3:/usr/local/hadoop/
     ```

     

     5. Hadoop 集群启动

     - HDFS 集群启动

         - node2

         ```shell
         hdfs --daemon start namenode
         hdfs --daemon start datanode
         ```

         - node1

         ```shell
         hdfs --daemon start datanode
         ```

         - node3

         ```shell
         hdfs --daemon start datanode
         ```

     - 

- yarn 集群启动
    - node3

    ```shell
    yarn --daemon start resourcemanager
    yarn --daemon start nodemanager
    ```

    - node2

    ```shell
    yarn --daemon start nodemanager
    ```

    - node1

    ```shell
    yarn --daemon start nodemanager
    ```

- secondNamenode  配置与启动
    
    - mapred-site.xml 增加配置

```shell
<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>node3:10020</value>
</property>
<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>node3:19888</value>
</property>
```

- 同步配置 rsync

- 启动 secondNamenode  

```shell
## 也可以通过其它方式启动
mr-jobhistory-daemon.sh start historyserver
```

- 配置日志聚集
    - yarn-site.xml

```shell
<!-- 日志聚集功能使能 -->
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>
<!-- 日志保留时间设置7天 -->
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
</property>
```

- 重新启动集群



6. 其它

- hadoop 默认配置

```shell
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\
https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml
https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml
```

