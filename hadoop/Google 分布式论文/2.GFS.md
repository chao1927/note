## 2. The Google File System

Google File System（GFS） 一个面向**大规模数据密集型应用**的，**可伸缩的分布式**应用。



#### 1. 设计思路

- 组件失效被认为是常态事件，而不是意外事件
- 以通常的标准衡量，我们的文件非常巨大
- 绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式
- 应用程序和文件系统 API 的协同设计提高了整个系统的灵活性



#### 2. 设计预期

###### 2.1  系统的主要工作负载

- 大规模流式读取操作
- 小规模的随机读取操作，可合并并排序，之后按顺序批量读取
- 大规模的顺序的数据追加方式的写入操作
- 小规模的随机位置写入操作

###### 2.2 实现多客户端并行追加数据到同一个文件

- 系统必须高效的、行为定义明确的2实现多客户端并行追加数据到同一个文件里的语意
- 使用最小的同步开销来实现的原子的多路追加数据操作是必 不可少的。文件可以在稍后读取，或者是消费者在追加的操作的同时读取文件。

###### 2.3 高性能的稳定网络带宽远比低延迟重要

- 目标程序绝大部分要求能够高速率的、大批量的处理数 据，极少有程序对单一的读写操作有严格的响应时间要求



#### 3. GFS 文件系统模型API

文件以分层目录的形式组织，用路径名来标识。我们支持常用的操作，如创建新文件、删除文件、打开 文件、关闭文件、读和写文件。

GFS 提供了快照和记录追加操作。快照以很低的成本创建一个文件或者目录树的拷贝。记录追加 操作允许多个客户端同时对一个文件进行数据追加操作，同时保

证每个客户端的追加操作都是原子性的。这 对于实现多路结果合并，以及“生产者-消费者”队列非常有用，多个客户端可以在不需要额外的同步锁定的 情况下，同

时对一个文件追加数据。



#### 4. GFS 系统架构

一个GFS集群包含一个单独的 Master 节点，多台 Chunk 服务器，并且同时被多个客户端访问。GFS 存储的文件被分割成固定大小的 Chunk。在 Chunk 创建的时

侯，Master 服务器会给每一个 Chunk 分配一个不变的全球唯一的 64 位的 Chunk 标识。Chunk 服务器把 Chunk 以文件的形式保存在本地磁盘中，并且根据指定

的 Chunk 标识和字节范围来读写块数据。（相当于将多台 Chunk 服务器的存储空间分割为固定大小并编号，方便数据存储空间管理）这样的话，每个 Chunk 可

以复制多个副本，写个多个 Chunk 服务器上，进行备份。（默认3个副本）



Master 节点管理所有的文件系统元数据，包括命名空间，访问控制信息，文件和 Chunk 的映射信息，以及当前 Chunk 的位置信息。还有管理 Chunk 服务节点。

Master 节点使用心跳信息周期地和每个 Chunk 服务器通讯，发送指令到各个 Chunk 服务器并接收 Chunk 服务器的状态信息。



GFS 客户端代码以库的形式被链接到客户程序里。客户端代码实现了 GFS 文件系统的 API 接口函数、 应用程序与 Master 节点和 Chunk 服务器通讯、以及对数据进行读写操作。客户端和 Master 节点的通信只获取 元数据，所有的数据操作都是由客户端直接和 Chunk 服务器进行交互的。



###### 4.1 单一 Master 节点

单一 Master 节点可以简化系统设计，优化整个分布式系统的元数据管理，和 Master 节点系统的管理行为。Chunk 服务器只需要向一个 Master 节点汇报信息。

客户端把文件名和程序指定的字节偏移，根据固定 的 Chunk 大小，转换成文件的 Chunk 索引。然后，它把文件名和 Chunk 索引发送给 Master 节点。Master 节 

点将相应的 Chunk 标识和副本的位置信息发还给客户端。客户端用文件名和 Chunk 索引作为 key 缓存这些信息。另外，客户端与 Master 节点询问 Chunk 节

点，之后与 Chunk 服务器，进行数据交互。这样可以避免 Master 称为分布式系统的性能瓶颈。



###### 4.2 Chunk 尺寸

每个 Chunk 的副本都以普通 Linux 文件的形式保存在 Chunk 服务器上，只有在需要的时候才扩大。惰性空间 分配策略避免了因内部碎片造成的空间浪费。

**如何选择 Chunk 的大小？**

- 考虑分配策略避免因内部碎片造成的空间浪费。
- 减少客户端和 Master 节点通讯的需求，因为只需要一次和 Master 节点的通信就可以获取 Chunk 的位置信息，之后就可以对同一个Chunk 进行多次的读写操作。
- 客户端能够对一个块进行多次操作，通过与Chunk 服务器保持较长时间的 TCP 连接来减少网络负载。
- 选用较大的Chunk尺寸减少了 Master 节点需要保存的元数据的数量。
- 当小文件包含较少的Chunk，甚至只有一个 Chunk，当许多客户端对同一个小文件进行多次的访问时，存储这些 Chunk 的 Chunk 服务器就会变成热点。



###### 4.3 元数据

- 文件和Chunk 的命名空间
- 文件和Chunk 的对应关系
- 每个Chunk 副本存放地点

前两种类型的元数据同时也会以记录变更日志的方式记录在操作系统的系统日志文件中，日志文件存储在本地磁盘上，同时日志会被复制到其它的远程Master服

务器上。采用保存变更日志的方式，我们能够简单可靠的更新 Master 服务器的状态，并且不用担心 Master 服务器崩溃导致数据不一致的风险。Master 服务器不

会持久保存 Chunk 位置信息。Master 服务器在启动时，或者有新的 Chunk 服务器加入时，向各个 Chunk 服务器轮询它们所存储的 Chunk 的信息。Master 服务

器能够保证它持有的信息始终是最新的，因为 它控制了所有的 Chunk 位置的分配，而且通过周期性的心跳信息监控 Chunk 服务器的状态。（只有 Chunk 才能确

定一个 Chunk 是否在它的硬盘上）

**操作日志：**

操作日志包含了关键的元数据变更历史记录。

1. 元数据持久化到操作日志，日志才对客户端可见。
2. 将操作日志复制到远程机器，且只有同时写入到本地和远程机器的硬盘后，才会响应客户端的请求。
3. 通过 Checkpoint ，减少日志的大小，这样也可以在恢复 Master 减少需要重演的日志。只需要恢复最近的 Checkpoint 及之后的日志。



###### 4.4 一致性模型

1. GFS 一致性保障机制

|          | 写             | 记录追加   |
| -------- | -------------- | ---------- |
| 串行成功 | 已定义         | 已定义     |
| 并行成功 | 一致但是未定义 | 部分不一致 |
| 失败     | 不一致         | 不一致     |

- 一致： 所有客户端，无论从哪个副本读取，读到的数据都一样
- 已定义：对文件的数据修改之后，region 是一致的，并且客户端能够看到写入操作全部的内容