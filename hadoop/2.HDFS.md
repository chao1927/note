## 二 Hadoop

Hadoop 的作用：

##### 数据存储与分析

当数据开始大量快速增长时，磁盘读取速度却没与时俱进。因此，可以采用多个硬盘进行存储，每个磁盘存储一部分，这样就可以并行读写，提高数据的访问速度。提升速度的同时也带来了一些问题。

1. 硬件故障问题，当硬件数量增加，发生问题的概率也增加了。通常可以采取**复制(replication)**的方式进行处理：系统在多个不同磁盘中保存数据的副本，一旦发生故障，就可以使用另外保存的副本。（HDFS）

2. 大多数数据分析任务需要以某种方式结合大部分数据来共同完成分析，即从一个硬盘读取的数据可能需要与其它99个硬盘读取的数据结合使用。如何使用保证正确性，可以使用 MapReduce 编程模型。MapReduce 是一个批量查询处理器，能够在合理的时间范围内处理整个数据集的动态查询。



##### 1. HDFS

HDFS 以流式数据访问模式来存储超大文件，运行于商用软件集群上。

- 超大文件

超过几百 MB, 几百 GB, 甚至是 PB 的存储

- 流式数据访问

一次写入,多次读取是最高效的访问方式, 数据集通常由数据源生成或者从数据源复制而来,接着长时间在此数据集上进行各种分析.

- 商用软件

- 低时间延迟的数据访问

HDFS 是为高数据吞吐量应用优化的,这可能会以提高时间延迟为代价.

- 大量的小文件

由于 namenode 将文件系统的元数据存储在内存中,因此该文件系统所能存储的文件总数受限于 namenode 的内存总量. 可以采用**联邦 HDFS**

- 多用户写入，任意修改文件

HDFS 中的文件写入只支持 APPEND 的方式.



##### 2. HDFS 介绍

![HDFS 架构流程](D:/note/hadoop/HDFS%20%E6%9E%B6%E6%9E%84%E6%B5%81%E7%A8%8B.png)



1. hdfs 概念介绍

- 数据块

每个磁盘都有默认的数据块大小, 这是磁盘进行数据读/写的最小单位.构建于单个磁盘之上的文件系统通过磁盘块来管理该文件系统中的块, 该文件系统块的大小可以是磁盘块的整数倍. HDFS 同样也有块 (block) 的概念,默认是 128MB, 与单一磁盘上的文件系统相似, HDFS 上的文件系统也被划分为块大小的多个分块 (chunk), 作为独立的存储单元. 但与面向单一磁盘的文件系统不同的是, HDFS 中小于一个块大小的文件不会占据整个块的空间 (例如, 当一个 1 MB 的文件存储在一个 128MB 的块中时, 文件只会使用 1MB 的磁盘空间, 而不是 128MB).

对于分布式文件系统中的块进行抽象会带来很多好处.

1. 一个文件的大小可以大于网络中的任意一个磁盘的容量.
2. 使用抽象块而非整个文件作为存储单元, 大大简化了存储子系统的设计.
3. 块还非常适合用于数据备份进而提供数据容错能力和提高可用性.



- **namenode 和 datanode**

    - **namenode 职责**

        1. 维护管理Hdfs的名称空间（namespace） 

        2. 维护副本策略 
        3. 记录文件块（Block）的映射信息 
        4. 负责处理客户端读写请求

    - **datanode 职责**
        1. 保存实际的数据块 
        2. 负责数据块的读写

HDFS 集群有两类阶段以管理节点-工作节点模式允许, 即一个 namenode (管理节点) 和多个 datanode (工作节点).

**namenode** 管理文件系统的命令空间. 它维护着文件系统树及整棵树内所有的文件和目录 (元数据管理). 这些信息以两个文件形式永久保存在本地磁盘上, **命令空间镜像文件 (fsimage)** 和**编辑日志文件 (editlog)**. namenode 也记录着每个文件各个块所在的数据节点信息, 但它并不永久保存块的位置信息 (不会持久化块的位置信息), 因为这些信息会在系统启动时根据数据节点信息重建.

客户端 (client) 代表用户通过与 namenode 和 datanode 交互来访问整个文件系统.客户端提供了一个类似于 POSIX 的文件系统接口, 因此用户在编程时, 无需知道namenode 和 datanode 也可实现功能.

datanode 是文件系统的工作节点. 它们根据需要存储并检索数据块 (受 Client 或 namenode 调度), 并定期向 namenode 发送它们所存储的块的列表.

namenode 是文件系统的管理节点, 为了对 namenode 实现容错, Hadoop 提供了两种容错机制

1. **备份组成文件系统元数据持久化状态的文件.**

    Hadoop 可以通过配置使 namenode 在多个文件系统上保存元数据的持久状态.这些写操作可以是实时同步且是原子操作. 一般的配置是,将持久状态写入本地磁盘的同时, 写入一个远程挂载的网络文件系统  (NFS).

2. **允许一个辅助 namenode (Secondary namenode)**

    辅助 namenode **定期合并编辑日志与命令空间镜像,以防止编辑日志过大**. 这个辅助 namenode 一般在另一个单独的物理计算机上允许, 因为它需要占用大量的 CPU 时间, 并且需要与 namenode 一样多的内存来执行合并操作, 它会保存合并过的命名空间镜像的副本,并在 namenode 发生故障时启用.但是, 辅助 namenode 保存的状态总是滞后于主节点, 所以在主节点全部失效时, 难免会丢失部分数据. 在这种情况下, 一般会把存储在 NFS 上 namenode 元数据复制到辅助 namenode 并作为新的 namenode 运行.



- 块缓存

通过 datanode 从磁盘中读取块, 但对于访问频繁的文件, 其对应的块可能被显示地缓存在 datanode 的内存中, 以**堆外缓存(off-heap block cache)**的形式存在. 默认情况下, 一个块仅缓存在一个 datanode 内存中, 当然可以针对每个文件配置 datanode 的数量. 在运行作业时, 可以通过块缓存的优势提高读操作性能.

用户或应用通过在**缓存池 (cache pool)**  中增加一个 cache directive 来告诉 namenode 需要缓存哪些文件以及缓存时间. 缓存池是一个用于管理缓存权限和资源使用的管理性分组.



- 联邦 HDFS

namenode 在内存中保存文件系统中每个文件和每个数据块的引用关系, 这意味着对于一个拥有大量文件的超大集群来说, 内存将成为限制系统横向扩展的瓶颈. 在2.x 发行版系列中引入的联邦 HDFS 允许系统通过添加 namenode 实现横向扩展, 其中每个 namenode 管理的文件系统命名空间的一部分. 例如, 一个 namenode 可能管理 /user 目录下的所有文件, 而另一个 namenode 可能管理 /share 目录下的所有文件.

在联邦环境下, 每个 namenode 维护一个命名空间卷 (namespace volume), 由命名空间的元数据和一个数据块池 (block pool) 组成,数据块池包含该命名空间下文件的所有数据块. 命名空间卷之间是相互独立的, 数据块池包含该命名空间下文件的所有数据块. 命名空间卷之间是相互独立的,两两之间并不相互通信, 甚至其中一个 namenode 的失效也不会影响由其它 namenode 维护的命名空间的可用性. 数据块池不再进行切分, 因此集群中的 datanode 需要注册到每个 namenode, 并且存储着来自多个数据块池中的数据块.



- HDFS 的高可用性

**如何保证 HDFS 的高可用性**

1. 如何避免单点失效问题 ?
2. 如何尽量少的丢失元数据?

启动一个拥有文件系统元数据副本的新的 namenode, 并配置 datanode 和客户端以便使用这个新的 namenode.

新的 namenode 会进入安全模式, 直到满足一下条件才能响应服务:

1. 将命名空间镜像文件 (Fsimages) 导入到内存中.
2. 重新执行编辑日志 (editlog)
3. 接受到足够多的来自 datanode 的数据块报告并退出安全模式.



Hadoop2 增加了对 HDFS 高可用性 (HA) 的支持. 在这一实现中, 配置了一个活动-备用 (active-standby) namenode. 当活动 namenode 失效,备用 namenode 就会接管它的任务并开始服务于来自客户端的请求, 不会有任何明细的中断. 实现以上目标需要在架构上做如下修改.

- namenode 之间需要通过高可用共享存储实现编辑日志(edit log) 的共享. 当备用 namenode 接管工作之后, 它将通过共享编辑日志直至末尾. 以实现活动 namenode 的状态同步, 并继续读取有活动 namenode 写入的新条目.
- datanode 需要同时向两个 namenode 发送数据块处理报告,因为数据块的映射信息存储在 namenode 的内存中, 而非磁盘.
- 客户端需要使用特定的机制来处理 namenode 的失效问题, 这一机制对用户是透明的.
- 辅助 namenode 的角色被备用 namenode 所包含, 备用 namenode 为活动的 namenode 命令空间设置周期性检查点.



**两种高可用性共享存储**

- NFS 过滤器

- 群体日志管理器 (QJM, quorum journal manager)

    QJM 是一个专用的 HDFS 实现, 为提供一个高可用的编辑日志而设计, 被推荐用于大多数 HDFS 部署中.

    QJM 以一组日志节点 (journal node) 的形式运行, 每一次编辑必须写入多数日志节点. 典型的,有三个 journal 节点, 所以系统能够忍受其中任何一个的丢失.



**故障的切换与规避**

系统中一个称为故障转移控制器 (failover contriller) 的新实体, 管理着将活动 namenode 转移为备用 namenode 的转换过程. 有多种故障转移控制器, 但默认的一种是使用了 Zookeeper 来确保有且仅有一个活动 namenode. 每一个 namenode 运行着一个轻量级的故障转移控制器, 其工作就是监视宿主机 namenode 是否失效并在 namenode 失效时进行故障切换.

另外, 对于客户端的故障转移通过客户端类库实现透明处理. 最简单的实现是通过客户端的配置文件实现故障转移的控制. 

1. 域名方式

2. 弹性IP 方式



##### 2. hdfs 写入数据

![Hadoop架构图-HDFS 读数据流程](D:/note/hadoop/Hadoop%E6%9E%B6%E6%9E%84%E5%9B%BE-HDFS%20%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png)



1. 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据， 找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。 
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。



##### 3. hdfs 读取数据

![Hadoop架构图-HDFS 写数据流程](D:/note/hadoop/Hadoop%E6%9E%B6%E6%9E%84%E5%9B%BE-HDFS%20%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png)



1. 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件 是否已存在，父目录是否存在。 
2. NameNode返回是否可以上传。 
3. 客户端请求第一个 Block上传到哪几个DataNode服务器上。 
4. NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。 
5. 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后 dn2调用dn3，将这个通信管道建立完成。 
6. dn1、dn2、dn3逐级应答客户端。 
7. 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单 位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个确认队列 等待确认。 
8. 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行 3-7步）。



##### 4. HDFS 操作命令

```shell
Usage: hadoop fs [generic options]
        [-appendToFile <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-checksum [-v] <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]
        [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]
        [-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]
        [-createSnapshot <snapshotDir> [<snapshotName>]]
        [-deleteSnapshot <snapshotDir> <snapshotName>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] [-v] [-x] <path> ...]
        [-expunge [-immediate] [-fs <path>]]
        [-find <path> ... <expression> ...]
        [-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-getfacl [-R] <path>]
        [-getfattr [-R] {-n name | -d} [-e en] <path>]
        [-getmerge [-nl] [-skip-empty-file] <src> <localdst>]
        [-head <file>]
        [-help [cmd ...]]
        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]
        [-renameSnapshot <snapshotDir> <oldName> <newName>]
        [-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
        [-setfattr {-n name [-v value] | -x name} <path>]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] [-s <sleep interval>] <file>]
        [-test -[defswrz] <path>]
        [-text [-ignoreCrc] <src> ...]
        [-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]
        [-touchz <path> ...]
        [-truncate [-w] <length> <path> ...]
        [-usage [cmd ...]]

Generic options supported are:
-conf <configuration file>        specify an application configuration file
-D <property=value>               define a value for a given property
-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.
-jt <local|resourcemanager:port>  specify a ResourceManager
-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster
-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath
-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines

```



##### 5. Hadoop 与 网络拓扑

在本地网络中, 两个节点被称为 "**彼此邻近**"是什么意思? 在海量数据处理中, 其主要限制因素是节点之间数据的传输速率-带宽很稀缺. 这里的想法是将两个节点之间的带宽作为距离的衡量标准.

不用衡量节点之间的带宽,实现上很难实现(它需要一个稳定的集权, 并且在集群中两两节点对数量是节点数量的平方),  hadoop 为此采用了一个简单的方法: 把网络看成一棵树, 两个节点之间的距离是它们到最近共同祖先的距离总和.该树中的层次是没有预先设定的, 但是, 相对于数据中心, 机架和正在运行的节点, 通常可以设定等级. 具体想法是针对以下每个场景, 可用带宽依次递减:

- 同一节点上的进程
- 同一机架上的不同节点
- 同一数据中心不同机架上的节点
- 不同数据中心中的节点

例如, 假设有数据中心 d1 机架 r1 中的节点 n1. 该节点可以表示为 /d1/r1/n1, 利用这种标记,这里可以给出四种距离描述:

- distance (/d1/r1/n1, /d1/r1/n1) = 0 **(同一节点上的进程)**

- distance (/d1/r1/n1, /d1/r1/n2) = 2 **(同一机架上的不同节点)**
- distance (/d1/r1/n1, /d1/r2/n3) = 4 **(同一数据中心不同机架上的节点)**
- distance (/d1/r1/n1, /d2/r3/n4) = 6 **(不同数据中心中的节点)**



##### 6. block 副本防止策略

Hadoop 的默认布局策略是在运行客户端的节点上放第1个副本 (如果客户端运行在集群之外, 就随机选择一个节点, 不过系统会避免挑选那些存储太满或者太忙的节点), 第2个副本放在和第1个不同且随机另外选择的机架中节点上 (离架). 第3个副本与第2个副本放在同一个机架上,且随机选择另一个节点. 其它副本放在集群中随机选择的节点上,不过系统会尽量避免在同一个机架上放置太多副本.



##### 7. 通过 distcp 并行复制

可以通过 distcp 对数据进行并行复制, 这种复制方式也是通过运行 MapReduce 作业方式来运行的, 只有 map 作业.